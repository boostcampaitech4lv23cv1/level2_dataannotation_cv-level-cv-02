{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os.path as osp\n",
    "import math\n",
    "import json\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from torch.utils.data import Dataset\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from custom_aug import ComposedTransformation\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def cal_distance(x1, y1, x2, y2):\n",
    "    '''calculate the Euclidean distance'''\n",
    "    return math.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
    "\n",
    "\n",
    "def move_points(vertices, index1, index2, r, coef):\n",
    "    '''move the two points to shrink edge\n",
    "    Input:\n",
    "        vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "        index1  : offset of point1\n",
    "        index2  : offset of point2\n",
    "        r       : [r1, r2, r3, r4] in paper\n",
    "        coef    : shrink ratio in paper\n",
    "    Output:\n",
    "        vertices: vertices where one edge has been shinked\n",
    "    '''\n",
    "    index1 = index1 % 4\n",
    "    index2 = index2 % 4\n",
    "    x1_index = index1 * 2 + 0\n",
    "    y1_index = index1 * 2 + 1\n",
    "    x2_index = index2 * 2 + 0\n",
    "    y2_index = index2 * 2 + 1\n",
    "\n",
    "    r1 = r[index1]\n",
    "    r2 = r[index2]\n",
    "    length_x = vertices[x1_index] - vertices[x2_index]\n",
    "    length_y = vertices[y1_index] - vertices[y2_index]\n",
    "    length = cal_distance(vertices[x1_index], vertices[y1_index], vertices[x2_index], vertices[y2_index])\n",
    "    if length > 1:\n",
    "        ratio = (r1 * coef) / length\n",
    "        vertices[x1_index] += ratio * (-length_x)\n",
    "        vertices[y1_index] += ratio * (-length_y)\n",
    "        ratio = (r2 * coef) / length\n",
    "        vertices[x2_index] += ratio * length_x\n",
    "        vertices[y2_index] += ratio * length_y\n",
    "    return vertices\n",
    "\n",
    "\n",
    "def shrink_poly(vertices, coef=0.3):\n",
    "    '''shrink the text region\n",
    "    Input:\n",
    "        vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "        coef    : shrink ratio in paper\n",
    "    Output:\n",
    "        v       : vertices of shrinked text region <numpy.ndarray, (8,)>\n",
    "    '''\n",
    "    x1, y1, x2, y2, x3, y3, x4, y4 = vertices\n",
    "    r1 = min(cal_distance(x1,y1,x2,y2), cal_distance(x1,y1,x4,y4))\n",
    "    r2 = min(cal_distance(x2,y2,x1,y1), cal_distance(x2,y2,x3,y3))\n",
    "    r3 = min(cal_distance(x3,y3,x2,y2), cal_distance(x3,y3,x4,y4))\n",
    "    r4 = min(cal_distance(x4,y4,x1,y1), cal_distance(x4,y4,x3,y3))\n",
    "    r = [r1, r2, r3, r4]\n",
    "\n",
    "    # obtain offset to perform move_points() automatically\n",
    "    if cal_distance(x1,y1,x2,y2) + cal_distance(x3,y3,x4,y4) > \\\n",
    "       cal_distance(x2,y2,x3,y3) + cal_distance(x1,y1,x4,y4):\n",
    "        offset = 0 # two longer edges are (x1y1-x2y2) & (x3y3-x4y4)\n",
    "    else:\n",
    "        offset = 1 # two longer edges are (x2y2-x3y3) & (x4y4-x1y1)\n",
    "\n",
    "    v = vertices.copy()\n",
    "    v = move_points(v, 0 + offset, 1 + offset, r, coef)\n",
    "    v = move_points(v, 2 + offset, 3 + offset, r, coef)\n",
    "    v = move_points(v, 1 + offset, 2 + offset, r, coef)\n",
    "    v = move_points(v, 3 + offset, 4 + offset, r, coef)\n",
    "    return v\n",
    "\n",
    "\n",
    "def get_rotate_mat(theta):\n",
    "    '''positive theta value means rotate clockwise'''\n",
    "    return np.array([[math.cos(theta), -math.sin(theta)], [math.sin(theta), math.cos(theta)]])\n",
    "\n",
    "\n",
    "def rotate_vertices(vertices, theta, anchor=None):\n",
    "    '''rotate vertices around anchor\n",
    "    Input:\n",
    "        vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "        theta   : angle in radian measure\n",
    "        anchor  : fixed position during rotation\n",
    "    Output:\n",
    "        rotated vertices <numpy.ndarray, (8,)>\n",
    "    '''\n",
    "    v = vertices.reshape((4,2)).T\n",
    "    if anchor is None:\n",
    "        anchor = v[:,:1]\n",
    "    rotate_mat = get_rotate_mat(theta)\n",
    "    res = np.dot(rotate_mat, v - anchor)\n",
    "    return (res + anchor).T.reshape(-1)\n",
    "\n",
    "\n",
    "def get_boundary(vertices):\n",
    "    '''get the tight boundary around given vertices\n",
    "    Input:\n",
    "        vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "    Output:\n",
    "        the boundary\n",
    "    '''\n",
    "    x1, y1, x2, y2, x3, y3, x4, y4 = vertices\n",
    "    x_min = min(x1, x2, x3, x4)\n",
    "    x_max = max(x1, x2, x3, x4)\n",
    "    y_min = min(y1, y2, y3, y4)\n",
    "    y_max = max(y1, y2, y3, y4)\n",
    "    return x_min, x_max, y_min, y_max\n",
    "\n",
    "\n",
    "def cal_error(vertices):\n",
    "    '''default orientation is x1y1 : left-top, x2y2 : right-top, x3y3 : right-bot, x4y4 : left-bot\n",
    "    calculate the difference between the vertices orientation and default orientation\n",
    "    Input:\n",
    "        vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "    Output:\n",
    "        err     : difference measure\n",
    "    '''\n",
    "    x_min, x_max, y_min, y_max = get_boundary(vertices)\n",
    "    x1, y1, x2, y2, x3, y3, x4, y4 = vertices\n",
    "    err = cal_distance(x1, y1, x_min, y_min) + cal_distance(x2, y2, x_max, y_min) + \\\n",
    "          cal_distance(x3, y3, x_max, y_max) + cal_distance(x4, y4, x_min, y_max)\n",
    "    return err\n",
    "\n",
    "\n",
    "def find_min_rect_angle(vertices):\n",
    "    '''find the best angle to rotate poly and obtain min rectangle\n",
    "    Input:\n",
    "        vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "    Output:\n",
    "        the best angle <radian measure>\n",
    "    '''\n",
    "    angle_interval = 1\n",
    "    angle_list = list(range(-90, 90, angle_interval))\n",
    "    area_list = []\n",
    "    for theta in angle_list:\n",
    "        rotated = rotate_vertices(vertices, theta / 180 * math.pi)\n",
    "        x1, y1, x2, y2, x3, y3, x4, y4 = rotated\n",
    "        temp_area = (max(x1, x2, x3, x4) - min(x1, x2, x3, x4)) * \\\n",
    "                    (max(y1, y2, y3, y4) - min(y1, y2, y3, y4))\n",
    "        area_list.append(temp_area)\n",
    "\n",
    "    sorted_area_index = sorted(list(range(len(area_list))), key=lambda k: area_list[k])\n",
    "    min_error = float('inf')\n",
    "    best_index = -1\n",
    "    rank_num = 10\n",
    "    # find the best angle with correct orientation\n",
    "    for index in sorted_area_index[:rank_num]:\n",
    "        rotated = rotate_vertices(vertices, angle_list[index] / 180 * math.pi)\n",
    "        temp_error = cal_error(rotated)\n",
    "        if temp_error < min_error:\n",
    "            min_error = temp_error\n",
    "            best_index = index\n",
    "    return angle_list[best_index] / 180 * math.pi\n",
    "\n",
    "\n",
    "def is_cross_text(start_loc, length, vertices):\n",
    "    '''check if the crop image crosses text regions\n",
    "    Input:\n",
    "        start_loc: left-top position\n",
    "        length   : length of crop image\n",
    "        vertices : vertices of text regions <numpy.ndarray, (n,8)>\n",
    "    Output:\n",
    "        True if crop image crosses text region\n",
    "    '''\n",
    "    if vertices.size == 0:\n",
    "        return False\n",
    "    start_w, start_h = start_loc\n",
    "    a = np.array([start_w, start_h, start_w + length, start_h, start_w + length, start_h + length,\n",
    "                  start_w, start_h + length]).reshape((4, 2))\n",
    "    p1 = Polygon(a).convex_hull\n",
    "    for vertice in vertices:\n",
    "        p2 = Polygon(vertice.reshape((4, 2))).convex_hull\n",
    "        inter = p1.intersection(p2).area\n",
    "        if 0.01 <= inter / p2.area <= 0.99:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def crop_img(img, vertices, labels, length):\n",
    "    '''crop img patches to obtain batch and augment\n",
    "    Input:\n",
    "        img         : PIL Image\n",
    "        vertices    : vertices of text regions <numpy.ndarray, (n,8)>\n",
    "        labels      : 1->valid, 0->ignore, <numpy.ndarray, (n,)>\n",
    "        length      : length of cropped image region\n",
    "    Output:\n",
    "        region      : cropped image region\n",
    "        new_vertices: new vertices in cropped region\n",
    "    '''\n",
    "\n",
    "    h, w = img.height, img.width\n",
    "    # confirm the shortest side of image >= length\n",
    "    if h >= w and w < length:\n",
    "        img = img.resize((length, int(h * length / w)), Image.BILINEAR)\n",
    "    elif h < w and h < length:\n",
    "        img = img.resize((int(w * length / h), length), Image.BILINEAR)\n",
    "    ratio_w = img.width / w\n",
    "    ratio_h = img.height / h\n",
    "    assert(ratio_w >= 1 and ratio_h >= 1)\n",
    "\n",
    "    new_vertices = np.zeros(vertices.shape)\n",
    "    if vertices.size > 0:\n",
    "        new_vertices[:,[0,2,4,6]] = vertices[:,[0,2,4,6]] * ratio_w\n",
    "        new_vertices[:,[1,3,5,7]] = vertices[:,[1,3,5,7]] * ratio_h\n",
    "\n",
    "    # find random position\n",
    "    remain_h = img.height - length\n",
    "    remain_w = img.width - length\n",
    "    flag = True\n",
    "    cnt = 0\n",
    "    while flag and cnt < 1000:\n",
    "        cnt += 1\n",
    "        start_w = int(np.random.rand() * remain_w)\n",
    "        start_h = int(np.random.rand() * remain_h)\n",
    "        flag = is_cross_text([start_w, start_h], length, new_vertices[labels==1,:])\n",
    "    box = (start_w, start_h, start_w + length, start_h + length)\n",
    "    region = img.crop(box)\n",
    "    if new_vertices.size == 0:\n",
    "        return region, new_vertices\n",
    "\n",
    "    new_vertices[:,[0,2,4,6]] -= start_w\n",
    "    new_vertices[:,[1,3,5,7]] -= start_h\n",
    "    return region, new_vertices\n",
    "\n",
    "\n",
    "\n",
    "def rand_crop_img(img, vertices, labels, length):\n",
    "    '''crop img patches to obtain batch and augment\n",
    "    Input:\n",
    "        img         : PIL Image\n",
    "        vertices    : vertices of text regions <numpy.ndarray, (n,8)>\n",
    "        labels      : 1->valid, 0->ignore, <numpy.ndarray, (n,)>\n",
    "        length      : length of cropped image region\n",
    "    Output:\n",
    "        region      : cropped image region\n",
    "        new_vertices: new vertices in cropped region\n",
    "    '''\n",
    "    n = np.random.choice([0,2,4,6,8,10])\n",
    "    length += n*32\n",
    "    h, w = img.height, img.width\n",
    "    # confirm the shortest side of image >= length\n",
    "    if h >= w and w < length:\n",
    "        img = img.resize((length, int(h * length / w)), Image.BILINEAR)\n",
    "    elif h < w and h < length:\n",
    "        img = img.resize((int(w * length / h), length), Image.BILINEAR)\n",
    "    ratio_w = img.width / w\n",
    "    ratio_h = img.height / h\n",
    "    assert(ratio_w >= 1 and ratio_h >= 1)\n",
    "\n",
    "    new_vertices = np.zeros(vertices.shape)\n",
    "    if vertices.size > 0:\n",
    "        new_vertices[:,[0,2,4,6]] = vertices[:,[0,2,4,6]] * ratio_w\n",
    "        new_vertices[:,[1,3,5,7]] = vertices[:,[1,3,5,7]] * ratio_h\n",
    "\n",
    "    # find random position\n",
    "    remain_h = img.height - length\n",
    "    remain_w = img.width - length\n",
    "    flag = True\n",
    "    cnt = 0\n",
    "    while flag and cnt < 1000:\n",
    "        cnt += 1\n",
    "        start_w = int(np.random.rand() * remain_w)\n",
    "        start_h = int(np.random.rand() * remain_h)\n",
    "        flag = is_cross_text([start_w, start_h], length, new_vertices[labels==1,:])\n",
    "    box = (start_w, start_h, start_w + length, start_h + length)\n",
    "    region = img.crop(box)\n",
    "    if new_vertices.size == 0:\n",
    "        return region, new_vertices\n",
    "\n",
    "    new_vertices[:,[0,2,4,6]] -= start_w\n",
    "    new_vertices[:,[1,3,5,7]] -= start_h\n",
    "    return region, new_vertices\n",
    "\n",
    "\n",
    "def rotate_all_pixels(rotate_mat, anchor_x, anchor_y, length):\n",
    "    '''get rotated locations of all pixels for next stages\n",
    "    Input:\n",
    "        rotate_mat: rotatation matrix\n",
    "        anchor_x  : fixed x position\n",
    "        anchor_y  : fixed y position\n",
    "        length    : length of image\n",
    "    Output:\n",
    "        rotated_x : rotated x positions <numpy.ndarray, (length,length)>\n",
    "        rotated_y : rotated y positions <numpy.ndarray, (length,length)>\n",
    "    '''\n",
    "    x = np.arange(length)\n",
    "    y = np.arange(length)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    x_lin = x.reshape((1, x.size))\n",
    "    y_lin = y.reshape((1, x.size))\n",
    "    coord_mat = np.concatenate((x_lin, y_lin), 0)\n",
    "    rotated_coord = np.dot(rotate_mat, coord_mat - np.array([[anchor_x], [anchor_y]])) + \\\n",
    "                                                   np.array([[anchor_x], [anchor_y]])\n",
    "    rotated_x = rotated_coord[0, :].reshape(x.shape)\n",
    "    rotated_y = rotated_coord[1, :].reshape(y.shape)\n",
    "    return rotated_x, rotated_y\n",
    "\n",
    "\n",
    "def resize_img(img, vertices, size):\n",
    "    h, w = img.height, img.width\n",
    "    ratio = size / max(h, w)\n",
    "    if w > h:\n",
    "        img = img.resize((size, int(h * ratio)), Image.BILINEAR)\n",
    "    else:\n",
    "        img = img.resize((int(w * ratio), size), Image.BILINEAR)\n",
    "    new_vertices = vertices * ratio\n",
    "    return img, new_vertices\n",
    "\n",
    "\n",
    "def resize_square_img(img, vertices, size):\n",
    "    h, w = img.height, img.width\n",
    "\n",
    "    ratio_h = size / h\n",
    "    ratio_w = size / w\n",
    "\n",
    "    img = img.resize((size,size), Image.BILINEAR)\n",
    "\n",
    "    new_vertices = vertices.copy()\n",
    "    new_vertices[:,[1,3,5,7]] = vertices[:,[1,3,5,7]]*ratio_h\n",
    "    new_vertices[:,[0,2,4,6]] = vertices[:,[0,2,4,6]]*ratio_w\n",
    "\n",
    "    return img, new_vertices\n",
    "\n",
    "\n",
    "def adjust_height(img, vertices, ratio=0.2):\n",
    "    '''adjust height of image to aug data\n",
    "    Input:\n",
    "        img         : PIL Image\n",
    "        vertices    : vertices of text regions <numpy.ndarray, (n,8)>\n",
    "        ratio       : height changes in [0.8, 1.2]\n",
    "    Output:\n",
    "        img         : adjusted PIL Image\n",
    "        new_vertices: adjusted vertices\n",
    "    '''\n",
    "    ratio_h = 1 + ratio * (np.random.rand() * 2 - 1)\n",
    "    old_h = img.height\n",
    "    new_h = int(np.around(old_h * ratio_h))\n",
    "    img = img.resize((img.width, new_h), Image.BILINEAR)\n",
    "\n",
    "    new_vertices = vertices.copy()\n",
    "    if vertices.size > 0:\n",
    "        new_vertices[:,[1,3,5,7]] = vertices[:,[1,3,5,7]] * (new_h / old_h)\n",
    "    return img, new_vertices\n",
    "\n",
    "\n",
    "def rotate_img(img, vertices, angle_range=10):\n",
    "    '''rotate image [-10, 10] degree to aug data\n",
    "    Input:\n",
    "        img         : PIL Image\n",
    "        vertices    : vertices of text regions <numpy.ndarray, (n,8)>\n",
    "        angle_range : rotate range\n",
    "    Output:\n",
    "        img         : rotated PIL Image\n",
    "        new_vertices: rotated vertices\n",
    "    '''\n",
    "    center_x = (img.width - 1) / 2\n",
    "    center_y = (img.height - 1) / 2\n",
    "    angle = angle_range * (np.random.rand() * 2 - 1)\n",
    "    img = img.rotate(angle, Image.BILINEAR)\n",
    "    new_vertices = np.zeros(vertices.shape)\n",
    "    for i, vertice in enumerate(vertices):\n",
    "        new_vertices[i,:] = rotate_vertices(vertice, -angle / 180 * math.pi, np.array([[center_x],[center_y]]))\n",
    "    return img, new_vertices\n",
    "\n",
    "\n",
    "def generate_roi_mask(image, vertices, labels):\n",
    "    mask = np.ones(image.shape[:2], dtype=np.float32)\n",
    "    ignored_polys = []\n",
    "    for vertice, label in zip(vertices, labels):\n",
    "        if label == 0:\n",
    "            ignored_polys.append(np.around(vertice.reshape((4, 2))).astype(np.int32))\n",
    "    cv2.fillPoly(mask, ignored_polys, 0)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def filter_vertices(vertices, labels, ignore_under=0, drop_under=0):\n",
    "    if drop_under == 0 and ignore_under == 0:\n",
    "        return vertices, labels\n",
    "\n",
    "    new_vertices, new_labels = vertices.copy(), labels.copy()\n",
    "\n",
    "    areas = np.array([Polygon(v.reshape((4, 2))).convex_hull.area for v in vertices])\n",
    "    labels[areas < ignore_under] = 0\n",
    "\n",
    "    if drop_under > 0:\n",
    "        passed = areas >= drop_under\n",
    "        new_vertices, new_labels = new_vertices[passed], new_labels[passed]\n",
    "\n",
    "    return new_vertices, new_labels\n",
    "\n",
    "\n",
    "def convert_tensor_to_PIL(image_tensor):\n",
    "    invTrans = A.Compose([A.Normalize(mean = [ 0., 0., 0. ],std = [ 1/0.229, 1/0.224, 1/0.225 ],max_pixel_value=1/255),A.Normalize(mean = [ -0.485, -0.456, -0.406 ],std = [ 1/255, 1/255, 1/255 ],max_pixel_value=255)])\n",
    "    invTrans = A.Compose([A.Normalize(mean = [ 0., 0., 0. ],std = [ 1, 1,1] ,max_pixel_value=1)])\n",
    "\n",
    "    img = Image.fromarray((invTrans(image = image_tensor))['image'].astype(np.uint8))\n",
    "    return img\n",
    "\n",
    "def plot_ground_truth(dataset_output0, dataset_output1):\n",
    "\n",
    "    img = convert_tensor_to_PIL(dataset_output0)\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    #ground_truth = source_val_json[\"images\"][IMG_NAME][\"words\"]\n",
    "    dataset_output1 = dataset_output1.astype(np.int32)\n",
    "\n",
    "    for points in  dataset_output1:\n",
    "    \n",
    "        points = points[::-1]\n",
    "        points = np.append(points,points[0]).reshape(-1,2)\n",
    "        for prev_pos, next_pos in zip(points[:-1], points[1:]):\n",
    "            ax.plot( [prev_pos[0], next_pos[0]], [prev_pos[1], next_pos[1]],color='r', linestyle='-', linewidth=1.5)\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(img)\n",
    "\n",
    "\n",
    "class SceneTextDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', image_size=1024, crop_size=512, color_jitter=True,\n",
    "                 normalize=True, to_gray=True, sharpen=False, clahe=True, sobel_probability=0.0):\n",
    "        with open(osp.join(root_dir, 'ufo/random_split/{}.json'.format(split)), 'r') as f:\n",
    "            anno = json.load(f)\n",
    "\n",
    "        self.anno = anno\n",
    "        self.image_fnames = sorted(anno['images'].keys())\n",
    "        self.image_dir = osp.join(root_dir, 'images')\n",
    "\n",
    "        self.split = split\n",
    "\n",
    "        self.image_size, self.crop_size = image_size, crop_size\n",
    "        self.color_jitter, self.normalize = color_jitter, normalize\n",
    "        self.to_gray = to_gray\n",
    "        self.sharpen = sharpen\n",
    "        self.clahe = clahe\n",
    "        self.sobel_probability = sobel_probability\n",
    "\n",
    "        funcs = []\n",
    "        if self.sharpen:\n",
    "            funcs.append(A.Sharpen(p=0.1))\n",
    "\n",
    "        if self.clahe:\n",
    "            funcs.append(A.CLAHE(p=0.5))\n",
    "\n",
    "        if self.to_gray:\n",
    "            funcs.append(A.ToGray(p=0.01))\n",
    "\n",
    "        if self.color_jitter:\n",
    "            funcs.append(A.ColorJitter(0.5, 0.5, 0.5, 0.25, p=0.5))\n",
    "\n",
    "        if self.normalize:\n",
    "            funcs.append(A.Normalize(mean = (0.5,0.5,0.5), std = (0.5,0.5,0.5)))\n",
    "        \n",
    "        self.transform = A.Compose(funcs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_fnames)\n",
    "\n",
    "    def __getitem__(self, lst):\n",
    "        idx, size_factor = lst[0], lst[1]\n",
    "\n",
    "        image_fname = self.image_fnames[idx]\n",
    "        image_fpath = osp.join(self.image_dir, image_fname)\n",
    "\n",
    "        vertices, labels = [], []\n",
    "        for word_info in self.anno['images'][image_fname]['words'].values():\n",
    "\n",
    "            points = np.array(word_info['points']).flatten()\n",
    "            check_point = 0 \n",
    "\n",
    "            # 8개넘어가면 안되게끔, polygon에 외접한 직사각형으로 수정!\n",
    "            if len(points) > 8 : \n",
    "                x = [round(points[i]) for i in range(len(points)) if i%2 ==0]\n",
    "                y = [round(points[i]) for i in range(len(points)) if i%2 !=0]\n",
    "                tmp_points = np.array([[x_pos, y_pos] for x_pos, y_pos in zip(x,y)])\n",
    "                rect = cv2.minAreaRect(tmp_points)\n",
    "                circum_box= cv2.boxPoints(rect)\n",
    "                points = np.array(circum_box).flatten()\n",
    "\n",
    "            # if check_point == 1: print('postprocess : ', len(points))\n",
    "            vertices.append(points)\n",
    "            labels.append(int(not word_info['illegibility']))\n",
    "\n",
    "        vertices, labels = np.array(vertices, dtype=np.float32), np.array(labels, dtype=np.int64)\n",
    "\n",
    "        vertices, labels = filter_vertices(vertices, labels, ignore_under=10, drop_under=1)\n",
    "\n",
    "        image = Image.open(image_fpath)\n",
    "\n",
    "        if self.split == 'train':\n",
    "            image, vertices = resize_img(image, vertices, self.image_size)\n",
    "            image, vertices = adjust_height(image, vertices)\n",
    "            image, vertices = rotate_img(image, vertices,10)\n",
    "            image, vertices = crop_img(image,vertices,labels,self.crop_size+(32*size_factor))\n",
    "            #image, vertices = rand_crop_img(image, vertices, labels, self.crop_size)\n",
    "            #image, vertices = resize_img(image, vertices, self.crop_size)\n",
    "\n",
    "        elif self.split == 'val':\n",
    "            image, vertices = resize_square_img(image, vertices, self.crop_size)\n",
    "\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        image = np.array(image)\n",
    "\n",
    "        word_bboxes = np.reshape(vertices, (-1, 4, 2))\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            if np.random.rand()<self.sobel_probability:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                image = cv2.Sobel(image, cv2.CV_64F, 0,1,ksize=-1)\n",
    "            else:\n",
    "                image = self.transform(image=image)['image']\n",
    "    \n",
    "        elif self.split == 'val':\n",
    "            image = A.Normalize(mean = (0.5,0.5,0.5), std = (0.5,0.5,0.5))(image = image)['image']\n",
    "\n",
    "        roi_mask = generate_roi_mask(image, vertices, labels)\n",
    "\n",
    "        return image, word_bboxes, roi_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "\n",
    "\n",
    "def shrink_bbox(bbox, coef=0.3, inplace=False):\n",
    "    lens = [np.linalg.norm(bbox[i] - bbox[(i + 1) % 4], ord=2) for i in range(4)]\n",
    "    r = [min(lens[(i - 1) % 4], lens[i]) for i in range(4)]\n",
    "\n",
    "    if not inplace:\n",
    "        bbox = bbox.copy()\n",
    "\n",
    "    offset = 0 if lens[0] + lens[2] > lens[1] + lens[3] else 1\n",
    "    for idx in [0, 2, 1, 3]:\n",
    "        p1_idx, p2_idx = (idx + offset) % 4, (idx + 1 + offset) % 4\n",
    "        p1p2 = bbox[p2_idx] - bbox[p1_idx]\n",
    "        dist = np.linalg.norm(p1p2)\n",
    "        if dist <= 1:\n",
    "            continue\n",
    "        bbox[p1_idx] += p1p2 / dist * r[p1_idx] * coef\n",
    "        bbox[p2_idx] -= p1p2 / dist * r[p2_idx] * coef\n",
    "    return bbox\n",
    "\n",
    "\n",
    "def get_rotated_coords(h, w, theta, anchor):\n",
    "    anchor = anchor.reshape(2, 1)\n",
    "    rotate_mat = get_rotate_mat(theta)\n",
    "    x, y = np.meshgrid(np.arange(w), np.arange(h))\n",
    "    x_lin = x.reshape((1, x.size))\n",
    "    y_lin = y.reshape((1, x.size))\n",
    "    coord_mat = np.concatenate((x_lin, y_lin), 0)\n",
    "    rotated_coord = np.dot(rotate_mat, coord_mat - anchor) + anchor\n",
    "    rotated_x = rotated_coord[0, :].reshape(x.shape)\n",
    "    rotated_y = rotated_coord[1, :].reshape(y.shape)\n",
    "    return rotated_x, rotated_y\n",
    "\n",
    "\n",
    "def get_rotate_mat(theta):\n",
    "    return np.array([[math.cos(theta), -math.sin(theta)],\n",
    "                     [math.sin(theta), math.cos(theta)]])\n",
    "\n",
    "\n",
    "def calc_error_from_rect(bbox):\n",
    "    '''\n",
    "    Calculate the difference between the vertices orientation and default orientation. Default\n",
    "    orientation is x1y1 : left-top, x2y2 : right-top, x3y3 : right-bot, x4y4 : left-bot\n",
    "    '''\n",
    "    x_min, y_min = np.min(bbox, axis=0)\n",
    "    x_max, y_max = np.max(bbox, axis=0)\n",
    "    rect = np.array([[x_min, y_min], [x_max, y_min], [x_max, y_max], [x_min, y_max]],\n",
    "                    dtype=np.float32)\n",
    "    return np.linalg.norm(bbox - rect, axis=0).sum()\n",
    "\n",
    "\n",
    "def rotate_bbox(bbox, theta, anchor=None):\n",
    "    points = bbox.T\n",
    "    if anchor is None:\n",
    "        anchor = points[:, :1]\n",
    "    rotated_points = np.dot(get_rotate_mat(theta), points - anchor) + anchor\n",
    "    return rotated_points.T\n",
    "\n",
    "\n",
    "def find_min_rect_angle(bbox, rank_num=10):\n",
    "    '''Find the best angle to rotate poly and obtain min rectangle\n",
    "    '''\n",
    "    areas = []\n",
    "    angles = np.arange(-90, 90) / 180 * math.pi\n",
    "    for theta in angles:\n",
    "        rotated_bbox = rotate_bbox(bbox, theta)\n",
    "        x_min, y_min = np.min(rotated_bbox, axis=0)\n",
    "        x_max, y_max = np.max(rotated_bbox, axis=0)\n",
    "        areas.append((x_max - x_min) * (y_max - y_min))\n",
    "\n",
    "    best_angle, min_error = -1, float('inf')\n",
    "    for idx in np.argsort(areas)[:rank_num]:\n",
    "        rotated_bbox = rotate_bbox(bbox, angles[idx])\n",
    "        error = calc_error_from_rect(rotated_bbox)\n",
    "        if error < min_error:\n",
    "            best_angle, min_error = angles[idx], error\n",
    "\n",
    "    return best_angle\n",
    "\n",
    "\n",
    "def generate_score_geo_maps(image, word_bboxes, map_scale=0.25):\n",
    "    img_h, img_w = image.shape[:2]\n",
    "    map_h, map_w = int(img_h * map_scale), int(img_w * map_scale)\n",
    "    inv_scale = int(1 / map_scale)\n",
    "\n",
    "    score_map = np.zeros((map_h, map_w, 1), np.float32)\n",
    "    geo_map = np.zeros((map_h, map_w, 5), np.float32)\n",
    "\n",
    "    word_polys = []\n",
    "\n",
    "    for bbox in word_bboxes:\n",
    "        poly = np.around(map_scale * shrink_bbox(bbox)).astype(np.int32)\n",
    "        word_polys.append(poly)\n",
    "\n",
    "        center_mask = np.zeros((map_h, map_w), np.float32)\n",
    "        cv2.fillPoly(center_mask, [poly], 1)\n",
    "\n",
    "        theta = find_min_rect_angle(bbox)\n",
    "        rotated_bbox = rotate_bbox(bbox, theta) * map_scale\n",
    "        x_min, y_min = np.min(rotated_bbox, axis=0)\n",
    "        x_max, y_max = np.max(rotated_bbox, axis=0)\n",
    "\n",
    "        anchor = bbox[0] * map_scale\n",
    "        rotated_x, rotated_y = get_rotated_coords(map_h, map_w, theta, anchor)\n",
    "\n",
    "        d1, d2 = rotated_y - y_min, y_max - rotated_y\n",
    "        d1[d1 < 0] = 0\n",
    "        d2[d2 < 0] = 0\n",
    "        d3, d4 = rotated_x - x_min, x_max - rotated_x\n",
    "        d3[d3 < 0] = 0\n",
    "        d4[d4 < 0] = 0\n",
    "        geo_map[:, :, 0] += d1 * center_mask * inv_scale\n",
    "        geo_map[:, :, 1] += d2 * center_mask * inv_scale\n",
    "        geo_map[:, :, 2] += d3 * center_mask * inv_scale\n",
    "        geo_map[:, :, 3] += d4 * center_mask * inv_scale\n",
    "        geo_map[:, :, 4] += theta * center_mask\n",
    "\n",
    "    cv2.fillPoly(score_map, word_polys, 1)\n",
    "\n",
    "    return score_map, geo_map\n",
    "\n",
    "\n",
    "class EASTDataset(Dataset):\n",
    "    def __init__(self, dataset, map_scale=0.25, to_tensor=True):\n",
    "        self.dataset = dataset\n",
    "        self.map_scale = map_scale\n",
    "        self.to_tensor = to_tensor\n",
    "\n",
    "    def __getitem__(self, lst):\n",
    "        image, word_bboxes, roi_mask = self.dataset[lst]\n",
    "        score_map, geo_map = generate_score_geo_maps(image, word_bboxes, map_scale=self.map_scale)\n",
    "\n",
    "        mask_size = int(image.shape[0] * self.map_scale), int(image.shape[1] * self.map_scale)\n",
    "        roi_mask = cv2.resize(roi_mask, dsize=mask_size)\n",
    "        if roi_mask.ndim == 2:\n",
    "            roi_mask = np.expand_dims(roi_mask, axis=2)\n",
    "\n",
    "        if self.to_tensor:\n",
    "            image = torch.Tensor(image).permute(2, 0, 1)\n",
    "            score_map = torch.Tensor(score_map).permute(2, 0, 1)\n",
    "            geo_map = torch.Tensor(geo_map).permute(2, 0, 1)\n",
    "            roi_mask = torch.Tensor(roi_mask).permute(2, 0, 1)\n",
    "\n",
    "        return image, score_map, geo_map, roi_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/opt/ml/input/data/ICDAR19'\n",
    "data = SceneTextDataset(DATA_PATH, split='train', image_size=1024, crop_size=512)\n",
    "dataset = EASTDataset(data)\n",
    "\n",
    "DATA_PATH = '/opt/ml/input/data/ICDAR17_Korean'\n",
    "data2 = SceneTextDataset(DATA_PATH, split='train', image_size=1024, crop_size=512)\n",
    "dataset2 = EASTDataset(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2[[0,2]][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0][0].shape)\n",
    "print(dataset[0][1].shape)\n",
    "print(dataset[0][2].shape)\n",
    "print(dataset[0][3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(new, batch_size=32, shuffle=True, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch2 = next(iter(train_loader))\n",
    "batch3 = next(iter(train_loader))\n",
    "batch4 = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 640, 640])\n",
      "torch.Size([32, 1, 160, 160])\n",
      "torch.Size([32, 5, 160, 160])\n",
      "torch.Size([32, 1, 160, 160])\n",
      "torch.Size([32, 3, 640, 640])\n",
      "torch.Size([32, 1, 160, 160])\n",
      "torch.Size([32, 5, 160, 160])\n",
      "torch.Size([32, 1, 160, 160])\n",
      "torch.Size([32, 3, 640, 640])\n",
      "640\n"
     ]
    }
   ],
   "source": [
    "print(batch[0].shape)\n",
    "print(batch[1].shape)\n",
    "print(batch[2].shape)\n",
    "print(batch[3].shape)\n",
    "print(batch2[0].shape)\n",
    "print(batch2[1].shape)\n",
    "print(batch2[2].shape)\n",
    "print(batch2[3].shape)\n",
    "print(batch3[0].shape)\n",
    "print(batch4[0].shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 640, 640])\n",
      "torch.Size([32, 3, 640, 640])\n",
      "torch.Size([32, 3, 640, 640])\n",
      "torch.Size([32, 3, 640, 640])\n",
      "torch.Size([32, 3, 640, 640])\n",
      "torch.Size([32, 3, 640, 640])\n",
      "torch.Size([32, 3, 640, 640])\n",
      "torch.Size([32, 3, 640, 640])\n",
      "torch.Size([32, 3, 640, 640])\n",
      "torch.Size([32, 3, 640, 640])\n",
      "torch.Size([32, 3, 704, 704])\n",
      "torch.Size([32, 3, 768, 768])\n",
      "torch.Size([32, 3, 640, 640])\n",
      "torch.Size([32, 3, 512, 512])\n",
      "torch.Size([32, 3, 832, 832])\n",
      "torch.Size([32, 3, 704, 704])\n",
      "torch.Size([32, 3, 512, 512])\n",
      "torch.Size([32, 3, 832, 832])\n",
      "torch.Size([32, 3, 832, 832])\n",
      "torch.Size([32, 3, 832, 832])\n",
      "torch.Size([32, 3, 640, 640])\n",
      "torch.Size([32, 3, 832, 832])\n",
      "torch.Size([32, 3, 512, 512])\n",
      "torch.Size([32, 3, 512, 512])\n",
      "torch.Size([32, 3, 576, 576])\n",
      "torch.Size([32, 3, 768, 768])\n",
      "torch.Size([32, 3, 576, 576])\n",
      "torch.Size([32, 3, 704, 704])\n",
      "torch.Size([32, 3, 832, 832])\n",
      "torch.Size([32, 3, 640, 640])\n",
      "torch.Size([32, 3, 832, 832])\n",
      "torch.Size([32, 3, 576, 576])\n",
      "torch.Size([32, 3, 640, 640])\n",
      "torch.Size([32, 3, 512, 512])\n",
      "torch.Size([32, 3, 768, 768])\n",
      "torch.Size([32, 3, 576, 576])\n",
      "torch.Size([32, 3, 640, 640])\n",
      "torch.Size([32, 3, 576, 576])\n",
      "torch.Size([32, 3, 832, 832])\n",
      "torch.Size([32, 3, 768, 768])\n",
      "torch.Size([32, 3, 768, 768])\n",
      "torch.Size([32, 3, 640, 640])\n",
      "torch.Size([32, 3, 704, 704])\n",
      "torch.Size([32, 3, 640, 640])\n",
      "torch.Size([32, 3, 512, 512])\n",
      "torch.Size([32, 3, 512, 512])\n",
      "torch.Size([32, 3, 768, 768])\n",
      "torch.Size([32, 3, 768, 768])\n",
      "torch.Size([32, 3, 832, 832])\n",
      "torch.Size([32, 3, 768, 768])\n",
      "torch.Size([32, 3, 832, 832])\n",
      "torch.Size([32, 3, 512, 512])\n",
      "torch.Size([32, 3, 768, 768])\n",
      "torch.Size([32, 3, 832, 832])\n",
      "torch.Size([32, 3, 704, 704])\n",
      "torch.Size([32, 3, 704, 704])\n",
      "torch.Size([32, 3, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "i= 0 \n",
    "for a,b,c,d in train_loader:\n",
    "    i+=1\n",
    "    print(a.shape)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  4 2020, 07:30:14) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
